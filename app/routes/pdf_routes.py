from __future__ import annotations

import io
import json
import re
from dataclasses import dataclass
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from flask import render_template_string, jsonify, request, send_file, abort

from app.app_dependencies import PDF_TO_HTML_AVAILABLE, extract_text_from_pdf
from app.app_helpers import get_source_files


def _split_merged_words(text: str) -> str:
    """
    –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —Å–ª—É—á–∞–µ–≤, –∫–æ–≥–¥–∞ —Å–ª–æ–≤–∞ –≤—Å—ë —Ä–∞–≤–Ω–æ —Å–ª–∏–ø–ª–∏—Å—å.
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –∫ –û–ß–ï–ù–¨ –¥–ª–∏–Ω–Ω—ã–º —Å–ª–æ–≤–∞–º (>50 —Å–∏–º–≤–æ–ª–æ–≤).
    """
    if not text or len(text) < 50:
        return text
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –û–ß–ï–ù–¨ –¥–ª–∏–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ (>50 —Å–∏–º–≤–æ–ª–æ–≤)
    words = text.split()
    very_long = [w for w in words if len(w) > 50]
    
    if not very_long:
        return text
    
    result = text
    
    # –¢–æ–ª—å–∫–æ —Å–∞–º—ã–µ –æ—á–µ–≤–∏–¥–Ω—ã–µ —Å–ª—É—á–∞–∏: "—Å–ª–æ–≤–æ.–°–ª–æ–≤–æ" ‚Üí "—Å–ª–æ–≤–æ. –°–ª–æ–≤–æ"
    result = re.sub(r'([–∞-—è—ëa-z])\.([–ê-–Ø–ÅA-Z])', r'\1. \2', result)
    
    # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
    result = re.sub(r' +', ' ', result)
    
    return result.strip()


def _find_annotation_block(page, lang: str, options: dict) -> tuple[float, float] | None:
    """
    ???????? ????? ???? ????????? ?? ???????? ?? ??????????:
    - RU: "?????????" ? ?? "???????? ?????"
    - EN: "Abstract" ? ?? "Keywords"
    ?????????? (top, bottom) ? ??????????? PDF (top/bottom).
    """
    try:
        words = page.extract_words(x_tolerance=1, y_tolerance=3, keep_blank_chars=False)
    except Exception:
        words = []

    if not words:
        return None

    line_tolerance = 3
    lines = []
    current_line = []
    current_top = None

    words_sorted = sorted(words, key=lambda w: (round(w["top"] / line_tolerance), w["x0"]))
    for w in words_sorted:
        if current_top is None:
            current_top = w["top"]
            current_line = [w]
        elif abs(w["top"] - current_top) <= line_tolerance:
            current_line.append(w)
        else:
            if current_line:
                line_top = min(x["top"] for x in current_line)
                line_bottom = max(x["bottom"] for x in current_line)
                line_text = " ".join(x["text"] for x in current_line)
                lines.append({"text": line_text, "top": line_top, "bottom": line_bottom})
            current_line = [w]
            current_top = w["top"]

    if current_line:
        line_top = min(x["top"] for x in current_line)
        line_bottom = max(x["bottom"] for x in current_line)
        line_text = " ".join(x["text"] for x in current_line)
        lines.append({"text": line_text, "top": line_top, "bottom": line_bottom})

    def _simplify_text(value: str) -> str:
        value = re.sub(r"\s+", " ", value).strip().lower()
        value = re.sub(r"[^\w\s]", " ", value, flags=re.UNICODE)
        return re.sub(r"\s+", " ", value).strip()

    heading_idx = None
    for i, line in enumerate(lines):
        text = _simplify_text(line["text"])
        if lang == "ru" and re.search(r"???????", text):
            heading_idx = i
            break
        if lang == "en" and re.search(r"abstract", text):
            heading_idx = i
            break

    if heading_idx is None:
        return None

    start_top = lines[heading_idx]["bottom"] + 2

    end_bottom = None
    for j in range(heading_idx + 1, len(lines)):
        text = _simplify_text(lines[j]["text"])
        if lang == "ru" and re.search(r"??????\w*\s*????\w*", text):
            end_bottom = lines[j]["top"] - 1
            break
        if lang == "en" and re.search(r"keywords?|key\s*words?|index\s*terms?", text):
            end_bottom = lines[j]["top"] - 1
            break

    max_ratio = float(options.get("annotation_max_height_ratio", 0.6))
    min_ratio = float(options.get("annotation_min_height_ratio", 0.12))
    min_height = page.height * min_ratio

    if end_bottom is not None:
        if (end_bottom - start_top) < min_height:
            end_bottom = None

    if end_bottom is None:
        footer_threshold = page.height * 0.85
        for line in reversed(lines[heading_idx + 1:]):
            if line["top"] < footer_threshold:
                break
            text = _simplify_text(line["text"])
            if re.fullmatch(r"\d{1,4}", text) or re.search(r"page|???", text):
                end_bottom = line["top"] - 1
                break

    if end_bottom is None:
        end_bottom = min(page.height, start_top + page.height * max_ratio)

    if end_bottom <= start_top:
        return None

    return start_top, end_bottom

def _is_garbled_text(text: str, lang: str | None = None) -> bool:
    """
    –ì—Ä—É–±–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —á—Ç–æ —Ç–µ–∫—Å—Ç "–∫–∞—à–∞".
    –û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞:
    - —Å—Ä–µ–¥–Ω–µ–π –¥–ª–∏–Ω–µ —Å–ª–æ–≤–∞
    - –Ω–∞–ª–∏—á–∏–∏ –∑–∞–≥–ª–∞–≤–Ω—ã—Ö –±—É–∫–≤ –≤–Ω—É—Ç—Ä–∏ —Å–ª–æ–≤–∞
    - –¥–æ–ª–µ —Å–ª–æ–≤ –±–µ–∑ –≥–ª–∞—Å–Ω—ã—Ö
    """
    if not text:
        return False

    words = [w for w in re.split(r"\s+", text) if w]
    if not words:
        return False

    avg_len = sum(len(w) for w in words) / len(words)
    if avg_len > 15:
        return True

    vowels_ru = set("–∞–µ—ë–∏–æ—É—ã—ç—é—è")
    vowels_en = set("aeiouy")

    internal_caps = 0
    mixed_case = 0
    no_vowel = 0
    low_vowel = 0

    for w in words:
        if len(w) > 3 and any(c.isupper() for c in w[1:]):
            internal_caps += 1
        if len(w) > 3 and any(c.isupper() for c in w[1:]) and any(c.islower() for c in w[1:]):
            mixed_case += 1

        lw = w.lower()
        has_ru = any(c in vowels_ru for c in lw)
        has_en = any(c in vowels_en for c in lw)
        if not (has_ru or has_en):
            no_vowel += 1
        else:
            # –î–æ–ª—è –≥–ª–∞—Å–Ω—ã—Ö –≤ —Å–ª–æ–≤–µ
            vcount = sum(1 for c in lw if c in vowels_ru or c in vowels_en)
            if len(lw) >= 5 and (vcount / len(lw)) < 0.2:
                low_vowel += 1

    caps_ratio = internal_caps / len(words)
    mixed_ratio = mixed_case / len(words)
    no_vowel_ratio = no_vowel / len(words)

    if caps_ratio > 0.05:
        return True
    if mixed_ratio > 0.05:
        return True
    if no_vowel_ratio > 0.3:
        return True
    if (low_vowel / len(words)) > 0.3:
        return True

    if lang == "ru" and no_vowel_ratio > 0.25:
        return True
    if lang == "en" and no_vowel_ratio > 0.3:
        return True

    return False


def _ocr_extract_text(page, bbox: tuple[float, float, float, float], lang: str) -> str | None:
    """OCR fallback. –¢—Ä–µ–±—É–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π pytesseract –∏ Tesseract OCR."""
    try:
        import pytesseract  # type: ignore
    except Exception:
        return None

    try:
        cropped = page.crop(bbox)
        img = cropped.to_image(resolution=300).original
        text = pytesseract.image_to_string(img, lang=lang)
        return text
    except Exception:
        return None


def _normalize_extracted_text(text: str, field_id: str | None, options: dict) -> str:
    if not text or text == "(–¢–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω)":
        return text
    cleaned = text.replace("\r\n", "\n").replace("\r", "\n")
    
    # –†–∞–∑–±–∏–≤–∞–µ–º —Å–ª–∏–ø—à–∏–µ—Å—è —Å–ª–æ–≤–∞ (–µ—Å–ª–∏ PDF –ø–ª–æ—Ö–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–ª—Å—è)
    cleaned = _split_merged_words(cleaned)

    fix_hyphenation = options.get("fix_hyphenation", True)
    if fix_hyphenation:
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã —Å –ø–µ—Ä–µ–≤–æ–¥–æ–º —Å—Ç—Ä–æ–∫–∏: "—Å–ª–æ-\n–≤–æ" ‚Üí "—Å–ª–æ–≤–æ"
        cleaned = re.sub(
            r"([A-Za-z–ê-–Ø–∞-—è–Å—ë])[-‚Äë‚Äì‚Äî]\s*\n\s*([a-z–∞-—è—ë])",
            r"\1\2",
            cleaned,
        )
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã —Å –ø—Ä–æ–±–µ–ª–æ–º (–∏–∑ PDF): "—Å–ª–æ- –≤–æ" ‚Üí "—Å–ª–æ–≤–æ"
        cleaned = re.sub(
            r"([A-Za-z–ê-–Ø–∞-—è–Å—ë])[-‚Äë‚Äì‚Äî]\s+([a-z–∞-—è—ë])",
            r"\1\2",
            cleaned,
        )

    strip_prefix = options.get("strip_prefix", True)
    if strip_prefix:
        if field_id in {"annotation", "annotation_en"}:
            cleaned = re.sub(r"^(–∞–Ω–Ω–æ—Ç–∞—Ü–∏—è|abstract)\s*[:\-]\s*", "", cleaned, flags=re.IGNORECASE)
        if field_id in {"keywords", "keywords_en"}:
            cleaned = re.sub(r"^(–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞|keywords)\s*[:\-]\s*", "", cleaned, flags=re.IGNORECASE)

    join_lines = options.get("join_lines")
    if join_lines is None:
        join_lines = field_id not in {"references_ru", "references_en"}
    if join_lines:
        cleaned = re.sub(r"[ \t]*\n[ \t]*", " ", cleaned)

    # –°–∫–ª–µ–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã: "–° –æ—Ä–≥" ‚Üí "–°_–æ—Ä–≥", "N –æ–±—â" ‚Üí "N_–æ–±—â"
    # –ü–∞—Ç—Ç–µ—Ä–Ω: –æ–¥–∏–Ω–æ—á–Ω–∞—è –±—É–∫–≤–∞ + –ø—Ä–æ–±–µ–ª + –∫–æ—Ä–æ—Ç–∫–æ–µ —Å–ª–æ–≤–æ (2-4 –±—É–∫–≤—ã, –æ–±—ã—á–Ω–æ –Ω–∏–∂–Ω–∏–π –∏–Ω–¥–µ–∫—Å)
    # –¢–∏–ø–∏—á–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã: –æ—Ä–≥, –æ–±—â, max, min, opt –∏ —Ç.–¥.
    cleaned = re.sub(
        r'\b([A-Z–ê-–Ø–Å])\s+(–æ—Ä–≥|–æ–±—â|max|min|opt|eff|tot|org|obs|calc|exp|—Ç–µ–æ—Ä|—ç–∫—Å–ø|—Å—Ä|–º–∏–Ω|–º–∞–∫—Å)\b',
        r'\1_\2',
        cleaned,
        flags=re.IGNORECASE
    )
    
    # –¢–∞–∫–∂–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–ª—É—á–∞–∏ –∫–æ–≥–¥–∞ –∏–Ω–¥–µ–∫—Å —É–∂–µ —á–∞—Å—Ç–∏—á–Ω–æ —Å–∫–ª–µ–µ–Ω: "–°–æ—Ä–≥" ‚Üí "–°_–æ—Ä–≥"  
    cleaned = re.sub(
        r'\b([CNPK–°])(\s*)(–æ—Ä–≥|–æ–±—â|org|tot)\b',
        r'\1_\3',
        cleaned,
        flags=re.IGNORECASE
    )

    cleaned = re.sub(r"[ \t]+", " ", cleaned).strip()
    return cleaned


class Language(Enum):
    """–Ø–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞."""

    RUSSIAN = "ru"
    ENGLISH = "en"
    MIXED = "mixed"
    UNKNOWN = "unknown"


@dataclass
class BBox:
    """–ü—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∞—è –æ–±–ª–∞—Å—Ç—å –≤ PDF."""

    x1: float
    y1: float
    x2: float
    y2: float

    def normalize(self) -> "BBox":
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã (x1 < x2, y1 < y2)."""
        return BBox(
            x1=min(self.x1, self.x2),
            y1=min(self.y1, self.y2),
            x2=max(self.x1, self.x2),
            y2=max(self.y1, self.y2),
        )

    def add_padding(self, padding_x: float, padding_y: float, page_width: float, page_height: float) -> "BBox":
        """–î–æ–±–∞–≤–ª—è–µ—Ç –æ—Ç—Å—Ç—É–ø—ã –∫ –æ–±–ª–∞—Å—Ç–∏."""
        return BBox(
            x1=max(0, self.x1 - padding_x),
            y1=max(0, self.y1 - padding_y),
            x2=min(page_width, self.x2 + padding_x),
            y2=min(page_height, self.y2 + padding_y),
        )

    def is_valid(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –æ–±–ª–∞—Å—Ç–∏."""
        return self.x1 < self.x2 and self.y1 < self.y2

    def to_tuple(self) -> Tuple[float, float, float, float]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—Ä—Ç–µ–∂ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –¥–ª—è pdfplumber."""
        return (self.x1, self.y1, self.x2, self.y2)


@dataclass
class ExtractionOptions:
    """–û–ø—Ü–∏–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞."""

    merge_by_field: bool = False
    annotation_by_heading: bool = True
    use_ocr_if_garbled: bool = False
    force_ocr: bool = False
    ocr_lang: Optional[str] = None
    fix_hyphenation: bool = True
    strip_prefix: bool = True
    join_lines: Optional[bool] = None
    annotation_max_height_ratio: float = 0.6
    annotation_min_height_ratio: float = 0.12
    padding_x: float = 5.0
    padding_y: float = 3.0
    line_tolerance: float = 5.0

    @classmethod
    def from_dict(cls, data: Optional[Dict[str, Any]]) -> "ExtractionOptions":
        """–°–æ–∑–¥–∞–µ—Ç –∏–∑ —Å–ª–æ–≤–∞—Ä—è."""
        if not data:
            return cls()
        return cls(
            merge_by_field=bool(data.get("merge_by_field", False)),
            annotation_by_heading=bool(data.get("annotation_by_heading", True)),
            use_ocr_if_garbled=bool(data.get("use_ocr_if_garbled", False)),
            force_ocr=bool(data.get("force_ocr", False)),
            ocr_lang=data.get("ocr_lang"),
            fix_hyphenation=bool(data.get("fix_hyphenation", True)),
            strip_prefix=bool(data.get("strip_prefix", True)),
            join_lines=data.get("join_lines"),
            annotation_max_height_ratio=float(data.get("annotation_max_height_ratio", 0.6)),
            annotation_min_height_ratio=float(data.get("annotation_min_height_ratio", 0.12)),
            padding_x=float(data.get("padding_x", 5.0)),
            padding_y=float(data.get("padding_y", 3.0)),
            line_tolerance=float(data.get("line_tolerance", 5.0)),
        )


class LanguageDetector:
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ —Ç–µ–∫—Å—Ç–∞."""

    @staticmethod
    def detect(text: str) -> Language:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞."""
        if not text:
            return Language.UNKNOWN

        cyrillic_count = sum(1 for c in text if 0x0400 <= ord(c) <= 0x04FF)
        latin_count = sum(1 for c in text if c.isalpha() and ord(c) < 0x0400)

        if cyrillic_count == 0 and latin_count == 0:
            return Language.UNKNOWN

        if cyrillic_count > 0 and latin_count > 0:
            return Language.RUSSIAN if cyrillic_count > latin_count else Language.ENGLISH

        if cyrillic_count > 0:
            return Language.RUSSIAN
        if latin_count > 0:
            return Language.ENGLISH

        return Language.UNKNOWN

    @staticmethod
    def has_cyrillic(text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –∫–∏—Ä–∏–ª–ª–∏—Ü—ã."""
        return any(0x0400 <= ord(c) <= 0x04FF for c in text)

    @staticmethod
    def has_latin(text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –ª–∞—Ç–∏–Ω–∏—Ü—ã."""
        return any(c.isalpha() and ord(c) < 0x0400 for c in text)


class TextQualityAnalyzer:
    """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""

    IDEAL_WORD_LENGTH = {
        Language.RUSSIAN: 6.5,
        Language.ENGLISH: 5.5,
        Language.MIXED: 6.0,
        Language.UNKNOWN: 6.0,
    }

    MIN_WORD_LENGTH = 3.0
    MAX_WORD_LENGTH = 15.0

    @staticmethod
    def calculate_average_word_length(text: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ä–µ–¥–Ω—é—é –¥–ª–∏–Ω—É —Å–ª–æ–≤–∞."""
        if not text:
            return float("inf")

        words = text.split()
        if not words:
            return float("inf")

        return sum(len(w) for w in words) / len(words)

    @staticmethod
    def calculate_short_words_ratio(text: str, threshold: int = 2) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –¥–æ–ª—é –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤."""
        if not text:
            return 1.0

        words = text.split()
        if not words:
            return 1.0

        short_count = sum(1 for w in words if len(w) <= threshold)
        return short_count / len(words)

    @classmethod
    def calculate_quality_score(cls, text: str, language: Language) -> float:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–∞.
        –ú–µ–Ω—å—à–µ = –ª—É—á—à–µ. 0 = –∏–¥–µ–∞–ª—å–Ω–æ.
        """
        if not text or text == "(–¢–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω)":
            return float("inf")

        avg_length = cls.calculate_average_word_length(text)
        ideal_length = cls.IDEAL_WORD_LENGTH.get(language, 6.0)

        if cls.MIN_WORD_LENGTH <= avg_length <= cls.MAX_WORD_LENGTH:
            score = abs(avg_length - ideal_length)
        else:
            score = 50.0 + abs(avg_length - ideal_length)

        short_ratio = cls.calculate_short_words_ratio(text)
        if short_ratio > 0.3:
            score += short_ratio * 20

        return score


class CharacterGapAnalyzer:
    """–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –º–µ–∂–¥—É —Å–∏–º–≤–æ–ª–∞–º–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ x_tolerance."""

    @dataclass
    class GapStatistics:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –º–µ–∂–¥—É —Å–∏–º–≤–æ–ª–∞–º–∏."""

        median: float
        p75: float
        p90: float
        min_gap: float
        max_gap: float
        suggested_x_tolerance: float
        suggested_space_threshold: Optional[float]

    @classmethod
    def analyze_gaps(cls, chars: List[Dict[str, Any]]) -> Optional["CharacterGapAnalyzer.GapStatistics"]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É —Å–∏–º–≤–æ–ª–∞–º–∏."""
        if not chars or len(chars) < 2:
            return None

        gaps = cls._extract_gaps(chars)
        if not gaps:
            return None

        gaps_sorted = sorted(gaps)
        n = len(gaps_sorted)

        stats = cls.GapStatistics(
            median=gaps_sorted[n // 2],
            p75=gaps_sorted[int(n * 0.75)],
            p90=gaps_sorted[int(n * 0.90)],
            min_gap=gaps_sorted[0],
            max_gap=gaps_sorted[-1],
            suggested_x_tolerance=0.0,
            suggested_space_threshold=None,
        )

        stats.suggested_x_tolerance, stats.suggested_space_threshold = cls._calculate_x_tolerance(
            stats, gaps_sorted
        )

        return stats

    @staticmethod
    def _extract_gaps(chars: List[Dict[str, Any]]) -> List[float]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É —Å–æ—Å–µ–¥–Ω–∏–º–∏ —Å–∏–º–≤–æ–ª–∞–º–∏ –Ω–∞ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ."""
        chars_sorted = sorted(chars, key=lambda c: (c.get("top", 0), c.get("x0", 0)))
        gaps = []

        for i in range(1, len(chars_sorted)):
            prev_char = chars_sorted[i - 1]
            curr_char = chars_sorted[i]
            if abs(prev_char.get("top", 0) - curr_char.get("top", 0)) < 5:
                gap = curr_char.get("x0", 0) - prev_char.get("x1", 0)
                if 0 < gap < 50:
                    gaps.append(gap)

        return gaps

    @staticmethod
    def _calculate_x_tolerance(
        stats: "CharacterGapAnalyzer.GapStatistics", gaps_sorted: List[float]
    ) -> Tuple[float, Optional[float]]:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π x_tolerance –∏ –ø–æ—Ä–æ–≥ –ø—Ä–æ–±–µ–ª–∞."""
        median = stats.median
        large_gaps = [g for g in gaps_sorted if g > median * 2.5]

        if large_gaps and len(large_gaps) >= 3:
            max_letter_gap = max(g for g in gaps_sorted if g <= median * 2.5)
            min_word_gap = min(large_gaps)
            space_threshold = (max_letter_gap + min_word_gap) / 2

            x_tolerance = min(space_threshold * 0.8, min_word_gap * 0.5)
            x_tolerance = max(0.5, x_tolerance)
            return x_tolerance, space_threshold

        x_tolerance = max(1.0, min(10.0, median * 1.5))
        return x_tolerance, None


class TextExtractor:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF –æ–±–ª–∞—Å—Ç–∏."""

    def __init__(self, options: ExtractionOptions):
        self.options = options
        self.quality_analyzer = TextQualityAnalyzer()

    def extract_from_crop(self, cropped_page, language_hint: Optional[Language] = None) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ –æ–±—Ä–µ–∑–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã PDF."""
        chars = getattr(cropped_page, "chars", None) or []
        gap_stats = CharacterGapAnalyzer.analyze_gaps(chars)

        if gap_stats:
            x_tolerance = gap_stats.suggested_x_tolerance
            print(f"DEBUG: x_tolerance={x_tolerance:.2f}, median_gap={gap_stats.median:.2f}")
        else:
            x_tolerance = 3.0
            print(f"DEBUG: x_tolerance={x_tolerance:.2f} (default)")

        candidates = self._try_extraction_methods(cropped_page, x_tolerance)
        best_text = self._select_best_candidate(candidates, language_hint)

        if gap_stats and self.quality_analyzer.calculate_average_word_length(best_text) > 20:
            reconstructed = self._reconstruct_from_chars(chars, gap_stats)
            if reconstructed:
                avg_reconstructed = self.quality_analyzer.calculate_average_word_length(reconstructed)
                avg_best = self.quality_analyzer.calculate_average_word_length(best_text)
                if avg_reconstructed < avg_best and avg_reconstructed < 25:
                    print(f"DEBUG: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (avg={avg_reconstructed:.1f})")
                    best_text = reconstructed

        return best_text or "(–¢–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω)"

    def _try_extraction_methods(self, cropped_page, x_tolerance: float) -> List[Tuple[str, str]]:
        """–ü—Ä–æ–±—É–µ—Ç —Ä–∞–∑–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞."""
        candidates = []

        try:
            try:
                text = cropped_page.extract_text(x_tolerance=x_tolerance, y_tolerance=5)
            except TypeError:
                text = cropped_page.extract_text()
            if text:
                candidates.append(("adaptive", text))
        except Exception as e:
            print(f"DEBUG: –ú–µ—Ç–æ–¥ 'adaptive' failed: {e}")

        try:
            try:
                text = cropped_page.extract_text(layout=True, x_tolerance=3, y_tolerance=5)
            except TypeError:
                text = cropped_page.extract_text()
            if text:
                text = re.sub(r" +", " ", text)
                text = re.sub(r"\n\s*\n+", "\n", text)
                candidates.append(("layout", text))
        except Exception as e:
            print(f"DEBUG: –ú–µ—Ç–æ–¥ 'layout' failed: {e}")

        try:
            try:
                text = cropped_page.extract_text(use_text_flow=True, x_tolerance=2, y_tolerance=5)
            except TypeError:
                text = cropped_page.extract_text()
            if text:
                candidates.append(("text_flow", text))
        except Exception as e:
            print(f"DEBUG: –ú–µ—Ç–æ–¥ 'text_flow' failed: {e}")

        try:
            try:
                text = cropped_page.extract_text(x_tolerance=0.5, y_tolerance=5)
            except TypeError:
                text = cropped_page.extract_text()
            if text:
                candidates.append(("tight", text))
        except Exception as e:
            print(f"DEBUG: –ú–µ—Ç–æ–¥ 'tight' failed: {e}")

        return candidates

    def _select_best_candidate(self, candidates: List[Tuple[str, str]], language_hint: Optional[Language]) -> str:
        """–í—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤."""
        if not candidates:
            return ""

        if language_hint is None:
            language_hint = LanguageDetector.detect(candidates[0][1])

        best_text = ""
        best_score = float("inf")
        best_method = ""

        for method, text in candidates:
            score = self.quality_analyzer.calculate_quality_score(text, language_hint)
            if _is_garbled_text(text, language_hint.value if language_hint != Language.UNKNOWN else None):
                score += 100

            if score < best_score:
                best_score = score
                best_text = text
                best_method = method

        avg_len = self.quality_analyzer.calculate_average_word_length(best_text)
        print(f"DEBUG: –õ—É—á—à–∏–π –º–µ—Ç–æ–¥: {best_method}, avg_word_len={avg_len:.1f}, score={best_score:.1f}")

        return best_text

    def _reconstruct_from_chars(
        self, chars: List[Dict[str, Any]], gap_stats: "CharacterGapAnalyzer.GapStatistics"
    ) -> Optional[str]:
        """–†–µ–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ —Å–∏–º–≤–æ–ª–æ–≤ –≤—Ä—É—á–Ω—É—é."""
        if not chars or not gap_stats.suggested_space_threshold:
            return None

        space_threshold = gap_stats.p90

        chars_sorted = sorted(chars, key=lambda c: (c.get("top", 0), c.get("x0", 0)))
        lines = []
        current_line = []
        current_top = None
        prev_char = None

        for char in chars_sorted:
            char_text = char.get("text", "")
            char_top = char.get("top", 0)
            char_x0 = char.get("x0", 0)

            if current_top is None or abs(char_top - current_top) > 5:
                if current_line:
                    lines.append("".join(current_line))
                current_line = [char_text]
                current_top = char_top
                prev_char = char
                continue

            if prev_char:
                gap = char_x0 - prev_char.get("x1", char_x0)
                if gap > space_threshold:
                    current_line.append(" ")

            current_line.append(char_text)
            prev_char = char

        if current_line:
            lines.append("".join(current_line))

        return "\n".join(lines)


class LanguageFilter:
    """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –ø–æ —è–∑—ã–∫—É."""

    @staticmethod
    def filter_by_language(
        text: str,
        words: List[Dict[str, Any]],
        target_language: Language,
        skip_filter: bool = False,
        line_tolerance: float = 5.0,
    ) -> str:
        """
        –§–∏–ª—å—Ç—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–∞ –Ω–∞ —Ü–µ–ª–µ–≤–æ–º —è–∑—ã–∫–µ.
        """
        if skip_filter or target_language == Language.UNKNOWN:
            return text

        if not words:
            return text

        detected = LanguageDetector.detect(text)
        if detected == target_language:
            return text

        if target_language == Language.RUSSIAN:
            filtered_words = [
                w
                for w in words
                if LanguageDetector.has_cyrillic(w["text"]) or not any(c.isalpha() for c in w["text"])
            ]
        elif target_language == Language.ENGLISH:
            filtered_words = [
                w
                for w in words
                if LanguageDetector.has_latin(w["text"]) or not any(c.isalpha() for c in w["text"])
            ]
        else:
            return text

        if not filtered_words:
            return text

        return LanguageFilter._reconstruct_text_from_words(filtered_words, line_tolerance=line_tolerance)

    @staticmethod
    def _reconstruct_text_from_words(words: List[Dict[str, Any]], line_tolerance: float = 5.0) -> str:
        """–í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ —Å–ø–∏—Å–∫–∞ —Å–ª–æ–≤ —Å —É—á–µ—Ç–æ–º —Å—Ç—Ä–æ–∫."""
        lines = []
        current_line = []
        current_top = None

        words_sorted = sorted(words, key=lambda w: (round(w["top"] / line_tolerance), w["x0"]))

        for word in words_sorted:
            word_top = word["top"]

            if current_top is None or abs(word_top - current_top) > line_tolerance:
                if current_line:
                    current_line.sort(key=lambda w: w["x0"])
                    lines.append(current_line)
                current_line = [word]
                current_top = word_top
            else:
                current_line.append(word)

        if current_line:
            current_line.sort(key=lambda w: w["x0"])
            lines.append(current_line)

        return "\n".join([" ".join([w["text"] for w in line]) for line in lines])


class PDFTextExtractor:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ –æ–±–ª–∞—Å—Ç–µ–π PDF."""

    RUSSIAN_FIELDS = {
        "title",
        "annotation",
        "keywords",
        "references_ru",
        "funding",
        "author_surname_rus",
        "author_initials_rus",
        "author_org_rus",
        "author_address_rus",
        "author_other_rus",
    }

    ENGLISH_FIELDS = {
        "title_en",
        "annotation_en",
        "keywords_en",
        "references_en",
        "funding_en",
        "author_surname_eng",
        "author_initials_eng",
        "author_org_eng",
        "author_address_eng",
        "author_other_eng",
    }

    MIXED_LANGUAGE_FIELDS = {"references_ru", "references_en", "annotation", "annotation_en"}

    def __init__(self, options: ExtractionOptions):
        self.options = options
        self.text_extractor = TextExtractor(options)
        self.language_filter = LanguageFilter()

    def extract_from_selection(self, page, selection: Dict[str, Any]) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ –æ–¥–Ω–æ–π –≤—ã–¥–µ–ª–µ–Ω–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏."""
        field_id = selection.get("field_id")
        page_num = selection.get("page", 0)

        target_language = self._get_target_language(field_id)
        skip_language_filter = field_id in self.MIXED_LANGUAGE_FIELDS

        bbox = self._get_bbox_from_selection(page, selection, field_id)

        if not bbox.is_valid():
            print(f"WARNING: –ù–µ–≤–∞–ª–∏–¥–Ω–∞—è –æ–±–ª–∞—Å—Ç—å –¥–ª—è {field_id}")
            return self._create_result(field_id, page_num, bbox, "(–ù–µ–≤–∞–ª–∏–¥–Ω–∞—è –æ–±–ª–∞—Å—Ç—å)")

        print(f"DEBUG: –û–±—Ä–∞–±–æ—Ç–∫–∞ {field_id} –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}")
        print(f"DEBUG: –û–±–ª–∞—Å—Ç—å: {bbox.to_tuple()}")

        try:
            cropped = page.crop(bbox.to_tuple())
        except Exception as e:
            print(f"ERROR: –û—à–∏–±–∫–∞ crop: {e}")
            return self._create_result(field_id, page_num, bbox, f"(–û—à–∏–±–∫–∞: {e})")

        text = self.text_extractor.extract_from_crop(cropped, target_language)

        try:
            try:
                words = cropped.extract_words(x_tolerance=3.0, y_tolerance=5.0, keep_blank_chars=False)
            except TypeError:
                words = cropped.extract_words()
        except Exception as e:
            print(f"DEBUG: –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Å–ª–æ–≤–∞: {e}")
            words = []

        if words and target_language != Language.UNKNOWN:
            text = self.language_filter.filter_by_language(
                text,
                words,
                target_language,
                skip_language_filter,
                line_tolerance=self.options.line_tolerance,
            )

        if self._should_use_ocr(text, field_id):
            ocr_text = self._apply_ocr(page, bbox, field_id)
            if ocr_text:
                text = ocr_text

        text = _normalize_extracted_text(text, field_id, self.options.__dict__)

        return self._create_result(field_id, page_num, bbox, text)

    def _get_target_language(self, field_id: Optional[str]) -> Language:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ü–µ–ª–µ–≤–æ–π —è–∑—ã–∫ –¥–ª—è –ø–æ–ª—è."""
        if not field_id:
            return Language.UNKNOWN

        if field_id in self.RUSSIAN_FIELDS:
            return Language.RUSSIAN
        if field_id in self.ENGLISH_FIELDS:
            return Language.ENGLISH
        return Language.UNKNOWN

    def _get_bbox_from_selection(self, page, selection: Dict[str, Any], field_id: Optional[str]) -> BBox:
        """–ü–æ–ª—É—á–∞–µ—Ç –æ–±–ª–∞—Å—Ç—å –∏–∑ –¥–∞–Ω–Ω—ã—Ö –≤—ã–¥–µ–ª–µ–Ω–∏—è."""
        def _has_manual_bbox(sel: Dict[str, Any]) -> bool:
            try:
                x1 = float(sel.get("pdf_x1", 0))
                y1 = float(sel.get("pdf_y1", 0))
                x2 = float(sel.get("pdf_x2", 0))
                y2 = float(sel.get("pdf_y2", 0))
            except (TypeError, ValueError):
                return False
            return abs(x2 - x1) > 1.0 and abs(y2 - y1) > 1.0

        has_manual = _has_manual_bbox(selection)

        if field_id in {"annotation", "annotation_en"} and self.options.annotation_by_heading and not has_manual:
            lang = "ru" if field_id == "annotation" else "en"
            block = _find_annotation_block(page, lang, self.options.__dict__)
            if block:
                top, bottom = block
                bbox = BBox(x1=0.0, y1=top, x2=page.width, y2=bottom)
                # –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤—ã–¥–µ–ª—è–ª –æ–±–ª–∞—Å—Ç—å, —Ä–∞—Å—à–∏—Ä—è–µ–º –Ω–∏–∑ –±–ª–æ–∫–∞ –¥–æ –≥—Ä–∞–Ω–∏—Ü –≤—ã–¥–µ–ª–µ–Ω–∏—è
                try:
                    sel_bbox = BBox(
                        x1=float(selection.get("pdf_x1", 0)),
                        y1=float(selection.get("pdf_y1", 0)),
                        x2=float(selection.get("pdf_x2", 0)),
                        y2=float(selection.get("pdf_y2", 0)),
                    ).normalize()
                    if sel_bbox.is_valid():
                        bbox = BBox(
                            x1=0.0,
                            y1=bbox.y1,
                            x2=page.width,
                            y2=max(bbox.y2, sel_bbox.y2),
                        )
                except Exception:
                    pass
                print(f"DEBUG: –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É: {bbox.to_tuple()}")
                return bbox.add_padding(self.options.padding_x, self.options.padding_y, page.width, page.height)

        bbox = BBox(
            x1=float(selection.get("pdf_x1", 0)),
            y1=float(selection.get("pdf_y1", 0)),
            x2=float(selection.get("pdf_x2", 0)),
            y2=float(selection.get("pdf_y2", 0)),
        )

        bbox = bbox.normalize()
        bbox = bbox.add_padding(self.options.padding_x, self.options.padding_y, page.width, page.height)
        return bbox

    def _should_use_ocr(self, text: str, field_id: Optional[str]) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å OCR."""
        if self.options.force_ocr:
            return True

        if not self.options.use_ocr_if_garbled:
            if field_id not in {"annotation", "annotation_en", "references_ru", "references_en"}:
                return False

        target_lang = self._get_target_language(field_id)
        lang_hint = target_lang.value if target_lang != Language.UNKNOWN else None
        return _is_garbled_text(text, lang_hint)

    def _apply_ocr(self, page, bbox: BBox, field_id: Optional[str]) -> Optional[str]:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç OCR –∫ –æ–±–ª–∞—Å—Ç–∏."""
        ocr_lang = self.options.ocr_lang
        if not ocr_lang:
            target_lang = self._get_target_language(field_id)
            if target_lang == Language.RUSSIAN:
                ocr_lang = "rus+eng"
            elif target_lang == Language.ENGLISH:
                ocr_lang = "eng+rus"
            else:
                ocr_lang = "rus+eng"

        print(f"DEBUG: –ü—Ä–∏–º–µ–Ω—è–µ–º OCR (lang={ocr_lang})")
        try:
            ocr_text = _ocr_extract_text(page, bbox.to_tuple(), ocr_lang)
            if ocr_text and ocr_text.strip():
                print(f"DEBUG: OCR —É—Å–ø–µ—à–Ω–æ: {ocr_text[:100]}")
                return ocr_text.strip()
        except Exception as e:
            print(f"DEBUG: OCR failed: {e}")

        return None

    @staticmethod
    def _create_result(field_id: Optional[str], page_num: int, bbox: BBox, text: str) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞."""
        return {
            "field_id": field_id,
            "page": page_num,
            "bbox": {"x1": bbox.x1, "y1": bbox.y1, "x2": bbox.x2, "y2": bbox.y2},
            "text": text,
        }

def register_pdf_routes(app, ctx):
    json_input_dir = ctx.get("json_input_dir")
    words_input_dir = ctx.get("words_input_dir")
    xml_output_dir = ctx.get("xml_output_dir")
    list_of_journals_path = ctx.get("list_of_journals_path")
    input_files_dir = ctx.get("input_files_dir")
    _input_files_dir = ctx.get("_input_files_dir")
    _words_input_dir = ctx.get("_words_input_dir")
    use_word_reader = ctx.get("use_word_reader")
    archive_root_dir = ctx.get("archive_root_dir")
    archive_retention_days = ctx.get("archive_retention_days")
    progress_state = ctx.get("progress_state")
    progress_lock = ctx.get("progress_lock")
    last_archive = ctx.get("last_archive")
    validate_zip_members = ctx.get("validate_zip_members")
    find_files_for_json = ctx.get("find_files_for_json")
    SUPPORTED_EXTENSIONS = ctx.get("SUPPORTED_EXTENSIONS")
    SUPPORTED_JSON_EXTENSIONS = ctx.get("SUPPORTED_JSON_EXTENSIONS")

    @app.route("/api/pdf-files")
    def api_pdf_files():
        """API endpoint –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ PDF —Ñ–∞–π–ª–æ–≤ –∏–∑ input_files (—Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ –≤–æ –≤—Å–µ—Ö –ø–æ–¥–ø–∞–ø–∫–∞—Ö)."""
        try:
            pdf_files = []
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º input_files_dir (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –∏–∑ –∑–∞–º—ã–∫–∞–Ω–∏—è)
            try:
                input_dir = _input_files_dir
                print(f"DEBUG: –ü—Ä–æ–≤–µ—Ä—è–µ–º input_files_dir: {input_dir}")
                print(f"DEBUG: input_files_dir type: {type(input_dir)}")
                print(f"DEBUG: input_files_dir exists: {input_dir.exists() if input_dir else 'None'}")
                print(f"DEBUG: input_files_dir is_dir: {input_dir.is_dir() if input_dir else 'None'}")
                
                if not input_dir or not input_dir.exists() or not input_dir.is_dir():
                    error_msg = f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è input_files –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {input_dir}"
                    print(f"ERROR: {error_msg}")
                    return jsonify({
                        "error": error_msg,
                        "input_files_dir": str(input_dir) if input_dir else "–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω"
                    }), 404
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ñ–∞–π–ª—ã
                pdf_count = len(list(input_dir.rglob("*.pdf")))
                print(f"DEBUG: ‚úÖ –ù–∞–π–¥–µ–Ω–æ PDF —Ñ–∞–π–ª–æ–≤ –≤ input_files: {pdf_count}")
                
            except NameError as ne:
                error_msg = f"input_files_dir –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω: {ne}"
                print(f"ERROR: {error_msg}")
                return jsonify({"error": error_msg}), 500
            except Exception as e:
                error_msg = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ input_files_dir: {e}"
                print(f"ERROR: {error_msg}")
                import traceback
                print(traceback.format_exc())
                return jsonify({"error": error_msg}), 500
            
            # –ò—â–µ–º PDF —Ñ–∞–π–ª—ã –≤ input_files
            print(f"DEBUG: üîç –ò—â–µ–º PDF —Ñ–∞–π–ª—ã –≤ input_files: {input_dir}")
            print(f"DEBUG: –ê–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å: {input_dir.resolve()}")
            file_count = 0
            for file_path in input_dir.rglob("*.pdf"):
                try:
                    file_count += 1
                    # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –æ—Ç –∫–æ—Ä–Ω–µ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                    relative = file_path.relative_to(input_dir)
                    file_entry = str(relative.as_posix())
                    pdf_files.append(file_entry)
                    print(f"DEBUG: ‚úÖ –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª #{file_count}: {file_entry} (–ø–æ–ª–Ω—ã–π –ø—É—Ç—å: {file_path})")
                except ValueError as ve:
                    print(f"DEBUG: ‚ùå –ü—Ä–æ–ø—É—â–µ–Ω —Ñ–∞–π–ª {file_path} –∏–∑-–∑–∞ ValueError: {ve}")
                    continue
            
            print(f"DEBUG: üìä –í input_files –Ω–∞–π–¥–µ–Ω–æ {file_count} PDF —Ñ–∞–π–ª–æ–≤")
            
            print(f"DEBUG: üéØ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(pdf_files)} PDF —Ñ–∞–π–ª–æ–≤")
            if len(pdf_files) == 0:
                print(f"DEBUG: ‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –§–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å:")
                print(f"DEBUG:   - input_files: {input_dir} (exists={input_dir.exists()}, is_dir={input_dir.is_dir()})")
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
            result = sorted(pdf_files)
            print(f"DEBUG: üì§ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ –∏–∑ {len(result)} —Ñ–∞–π–ª–æ–≤")
            return jsonify(result)
        except Exception as e:
            import traceback
            error_msg = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ PDF —Ñ–∞–π–ª–æ–≤: {str(e)}\n{traceback.format_exc()}"
            print(f"ERROR: {error_msg}")
            return jsonify({"error": str(e), "details": traceback.format_exc()}), 500
    

    @app.route("/pdf-bbox")
    def pdf_bbox_form():
        """–í–µ–±-—Ñ–æ—Ä–º–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ bbox –≤ PDF —Ñ–∞–π–ª–∞—Ö."""
        return render_template_string(PDF_BBOX_TEMPLATE)
    

    @app.route("/api/pdf-bbox", methods=["POST"])
    def api_pdf_bbox():
        """API endpoint –¥–ª—è –ø–æ–∏—Å–∫–∞ –±–ª–æ–∫–æ–≤ –≤ PDF –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º."""
        try:
            data = request.get_json()
            pdf_filename = data.get("pdf_file")
            search_terms = data.get("terms", [])
            find_annotation = data.get("annotation", False)
            
            if not pdf_filename:
                return jsonify({"error": "–ù–µ —É–∫–∞–∑–∞–Ω —Ñ–∞–π–ª PDF"}), 400
            
            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å: –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø—É—Ç—å
            if ".." in pdf_filename or pdf_filename.startswith("/") or pdf_filename.startswith("\\"):
                return jsonify({"error": "–ù–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É"}), 400
            
            # –§–∞–π–ª –∏–∑ input_files
            pdf_path = _input_files_dir / pdf_filename
            base_dir = _input_files_dir
            
            if not pdf_path.exists() or not pdf_path.is_file():
                return jsonify({"error": f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {pdf_filename}"}), 404
            
            if pdf_path.suffix.lower() != ".pdf":
                return jsonify({"error": "–§–∞–π–ª –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å PDF"}), 400
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ –±–∞–∑–æ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            try:
                pdf_path.resolve().relative_to(base_dir.resolve())
            except ValueError:
                return jsonify({"error": "–ù–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É"}), 400
            
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å bbox
            try:
                from converters.pdf_to_html import find_text_blocks_with_bbox, find_annotation_bbox_auto
            except ImportError:
                return jsonify({"error": "–§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å bbox –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã"}), 500
            
            if find_annotation:
                # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
                result = find_annotation_bbox_auto(pdf_path)
                if result:
                    return jsonify({
                        "success": True,
                        "blocks": [result]
                    })
                else:
                    return jsonify({
                        "success": False,
                        "message": "–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
                    })
            else:
                # –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
                if not search_terms:
                    search_terms = ["–†–µ–∑—é–º–µ", "–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è", "Abstract", "Annotation", "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞", "Keywords"]
                
                blocks = find_text_blocks_with_bbox(
                    pdf_path,
                    search_terms=search_terms,
                    expand_bbox=(0, -10, 0, 100)
                )
                
                return jsonify({
                    "success": True,
                    "blocks": blocks
                })
        
        except Exception as e:
            return jsonify({"error": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {str(e)}"}), 500
    

    @app.route("/api/pdf-info", methods=["POST"])
    def api_pdf_info():
        """API endpoint –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ PDF (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü, —Ä–∞–∑–º–µ—Ä—ã)."""
        try:
            data = request.get_json()
            pdf_filename = data.get("pdf_file")
            
            if not pdf_filename:
                return jsonify({"error": "–ù–µ —É–∫–∞–∑–∞–Ω —Ñ–∞–π–ª PDF"}), 400
            
            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å: –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø—É—Ç—å
            if ".." in pdf_filename or pdf_filename.startswith("/") or pdf_filename.startswith("\\"):
                return jsonify({"error": "–ù–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É"}), 400
            
            # –§–∞–π–ª –∏–∑ input_files
            pdf_path = _input_files_dir / pdf_filename
            base_dir = _input_files_dir
            
            if not pdf_path.exists() or not pdf_path.is_file():
                return jsonify({"error": f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {pdf_filename}"}), 404
            
            if pdf_path.suffix.lower() != ".pdf":
                return jsonify({"error": "–§–∞–π–ª –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å PDF"}), 400
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ –±–∞–∑–æ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            try:
                pdf_path.resolve().relative_to(base_dir.resolve())
            except ValueError:
                return jsonify({"error": "–ù–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É"}), 400
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ PDF —á–µ—Ä–µ–∑ pdfplumber
            try:
                import pdfplumber
                print(f"DEBUG: –û—Ç–∫—Ä—ã–≤–∞—é PDF: {pdf_path}")
                with pdfplumber.open(str(pdf_path)) as pdf:
                    pages_info = []
                    for page in pdf.pages:
                        pages_info.append({
                            "width": page.width,
                            "height": page.height
                        })
                    
                    print(f"DEBUG: PDF —Å–æ–¥–µ—Ä–∂–∏—Ç {len(pages_info)} —Å—Ç—Ä–∞–Ω–∏—Ü")
                    return jsonify({
                        "success": True,
                        "pdf_file": pdf_filename,
                        "total_pages": len(pages_info),
                        "pages": pages_info
                    })
            except ImportError as e:
                print(f"ERROR: pdfplumber –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {e}")
                return jsonify({"error": "pdfplumber –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω"}), 500
            except Exception as e:
                import traceback
                error_msg = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ PDF: {str(e)}\n{traceback.format_exc()}"
                print(f"ERROR: {error_msg}")
                return jsonify({"error": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ PDF: {str(e)}"}), 500
        
        except Exception as e:
            return jsonify({"error": f"–û—à–∏–±–∫–∞: {str(e)}"}), 500
    

    @app.route("/api/pdf-image/<path:pdf_filename>")
    def api_pdf_image(pdf_filename: str):
        """API endpoint –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã PDF."""
        try:
            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å: –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø—É—Ç—å
            if ".." in pdf_filename or pdf_filename.startswith("/") or pdf_filename.startswith("\\"):
                print(f"ERROR: –ù–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π –ø—É—Ç—å: {pdf_filename}")
                abort(404)
            
            # –§–∞–π–ª –∏–∑ input_files
            pdf_path = _input_files_dir / pdf_filename
            base_dir = _input_files_dir
            
            if not pdf_path.exists() or not pdf_path.is_file():
                print(f"ERROR: –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {pdf_path}")
                abort(404)
            
            if pdf_path.suffix.lower() != ".pdf":
                print(f"ERROR: –ù–µ PDF —Ñ–∞–π–ª: {pdf_path}")
                abort(404)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ –±–∞–∑–æ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            try:
                pdf_path.resolve().relative_to(base_dir.resolve())
            except ValueError:
                print(f"ERROR: –§–∞–π–ª –≤–Ω–µ –±–∞–∑–æ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {pdf_path}")
                abort(404)
            
            # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ query –ø–∞—Ä–∞–º–µ—Ç—Ä–∞
            page_num = request.args.get('page', '0')
            try:
                page_num = int(page_num)
            except ValueError:
                page_num = 0
            
            print(f"DEBUG: –ó–∞–ø—Ä–æ—Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num} –∏–∑ {pdf_filename}")
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É PDF –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            try:
                from pdf2image import convert_from_path
                print(f"DEBUG: pdf2image –¥–æ—Å—Ç—É–ø–µ–Ω, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_num + 1}")
                images = convert_from_path(
                    str(pdf_path),
                    first_page=page_num + 1,
                    last_page=page_num + 1,
                    dpi=150
                )
                
                if not images:
                    print(f"ERROR: –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num + 1}")
                    abort(404)
                
                print(f"DEBUG: –ü–æ–ª—É—á–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–º {images[0].size}")
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π –±—É—Ñ–µ—Ä
                from io import BytesIO
                img_buffer = BytesIO()
                images[0].save(img_buffer, format='PNG')
                img_buffer.seek(0)
                
                return send_file(img_buffer, mimetype='image/png')
            except ImportError as e:
                print(f"ERROR: pdf2image –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {e}")
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ 1x1 –ø–∏–∫—Å–µ–ª—å –≤–º–µ—Å—Ç–æ –æ—à–∏–±–∫–∏
                from io import BytesIO
                from PIL import Image
                img = Image.new('RGB', (1, 1), color='white')
                img_buffer = BytesIO()
                img.save(img_buffer, format='PNG')
                img_buffer.seek(0)
                return send_file(img_buffer, mimetype='image/png')
            except Exception as e:
                import traceback
                error_msg = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏: {str(e)}\n{traceback.format_exc()}"
                print(f"ERROR: {error_msg}")
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ –æ—à–∏–±–∫–∏
                from io import BytesIO
                from PIL import Image
                img = Image.new('RGB', (1, 1), color='white')
                img_buffer = BytesIO()
                img.save(img_buffer, format='PNG')
                img_buffer.seek(0)
                return send_file(img_buffer, mimetype='image/png')
        
        except Exception as e:
            import traceback
            error_msg = f"–û—à–∏–±–∫–∞: {str(e)}\n{traceback.format_exc()}"
            print(f"ERROR: {error_msg}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ –æ—à–∏–±–∫–∏
            try:
                from io import BytesIO
                from PIL import Image
                img = Image.new('RGB', (1, 1), color='white')
                img_buffer = BytesIO()
                img.save(img_buffer, format='PNG')
                img_buffer.seek(0)
                return send_file(img_buffer, mimetype='image/png')
            except:
                abort(500)
    

    @app.route("/api/pdf-extract-text", methods=["POST"])
    def api_pdf_extract_text():
        """API endpoint ??? ?????????? ?????? ?? ?????????? ???????? PDF."""
        try:
            data = request.get_json()
            pdf_filename = data.get("pdf_file")
            selections = data.get("selections", [])
            options_dict = data.get("options", {}) or {}

            print(f"DEBUG: ?????? ?????????? ?????? ?? {len(selections)} ????????")
            print(f"DEBUG: PDF ????: {pdf_filename}")

            if not pdf_filename:
                return jsonify({"error": "?? ?????? ???? PDF"}), 400

            if not selections:
                return jsonify({"error": "??? ?????????? ????????"}), 400

            if ".." in pdf_filename or pdf_filename.startswith("/") or pdf_filename.startswith("\\"):
                return jsonify({"error": "???????????? ???? ? ?????"}), 400

            pdf_path = _input_files_dir / pdf_filename

            if not pdf_path.exists() or not pdf_path.is_file():
                print(f"ERROR: ???? ?? ??????: {pdf_path}")
                return jsonify({"error": f"???? ?? ??????: {pdf_filename}"}), 404

            try:
                pdf_path.resolve().relative_to(_input_files_dir.resolve())
            except ValueError:
                return jsonify({"error": "???????????? ???? ? ?????"}), 400

            options = ExtractionOptions.from_dict(options_dict)

            try:
                import pdfplumber

                extractor = PDFTextExtractor(options)
                extracted = []

                print(f"DEBUG: ???????? PDF: {pdf_path}")
                with pdfplumber.open(str(pdf_path)) as pdf:
                    print(f"DEBUG: PDF ???????? {len(pdf.pages)} ???????")

                    for selection in selections:
                        page_num = selection.get("page", 0)
                        if page_num >= len(pdf.pages):
                            print(f"WARNING: ???????? {page_num} ?? ??????????")
                            continue

                        page = pdf.pages[page_num]
                        result = extractor.extract_from_selection(page, selection)
                        extracted.append(result)

                print(f"DEBUG: ????????? ?????? ?? {len(extracted)} ????????")

                merged = {}
                if options.merge_by_field:
                    for item in sorted(
                        extracted, key=lambda i: (i.get("field_id") or "", i.get("page", 0))
                    ):
                        field = item.get("field_id")
                        text = item.get("text")
                        if not field or not text or text == "(????? ?? ??????)":
                            continue
                        merged.setdefault(field, [])
                        merged[field].append(text)
                    merged = {k: "\n".join(v).strip() for k, v in merged.items()}

                return jsonify(
                    {
                        "success": True,
                        "extracted": extracted,
                        "merged": merged if options.merge_by_field else None,
                    }
                )

            except ImportError as e:
                print(f"ERROR: pdfplumber ?? ??????????: {e}")
                return jsonify({"error": "pdfplumber ?? ??????????"}), 500
            except Exception as e:
                import traceback

                error_msg = f"?????? ??? ?????????? ??????: {str(e)}\n{traceback.format_exc()}"
                print(f"ERROR: {error_msg}")
                return jsonify({"error": f"?????? ??? ?????????? ??????: {str(e)}"}), 500

        except Exception as e:
            import traceback

            error_msg = f"??????: {str(e)}\n{traceback.format_exc()}"
            print(f"ERROR: {error_msg}")
            return jsonify({"error": f"??????: {str(e)}"}), 500

    @app.route("/api/pdf-save-coordinates", methods=["POST"])
    def api_pdf_save_coordinates():
        """API endpoint –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –≤—ã–¥–µ–ª–µ–Ω–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π –≤ JSON —Ñ–∞–π–ª."""
        try:
            data = request.get_json()
            pdf_filename = data.get("pdf_file")
            total_pages = data.get("total_pages", 0)
            selections = data.get("selections", [])
            
            if not pdf_filename:
                return jsonify({"error": "–ù–µ —É–∫–∞–∑–∞–Ω —Ñ–∞–π–ª PDF"}), 400
            
            if not selections:
                return jsonify({"error": "–ù–µ—Ç –≤—ã–¥–µ–ª–µ–Ω–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è"}), 400
            
            # –°–æ–∑–¥–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
            pdf_path = Path(pdf_filename)
            output_filename = pdf_path.stem + "_bbox.json"
            output_path = json_input_dir / output_filename
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            output_data = {
                "pdf_file": pdf_filename,
                "total_pages": total_pages,
                "selections": []
            }
            
            for selection in selections:
                output_data["selections"].append({
                    "page": selection["page"],
                    "bbox": {
                        "x1": round(selection["pdf_x1"], 2),
                        "y1": round(selection["pdf_y1"], 2),
                        "x2": round(selection["pdf_x2"], 2),
                        "y2": round(selection["pdf_y2"], 2)
                    },
                    "text": selection.get("text", ""),
                    "field_id": selection.get("field_id")
                })
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            return jsonify({
                "success": True,
                "file_path": str(output_path),
                "file_name": output_filename
            })
        
        except Exception as e:
            return jsonify({"error": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏: {str(e)}"}), 500
    

    @app.route("/pdf/<path:pdf_filename>")
    def serve_pdf(pdf_filename: str):
        """–ú–∞—Ä—à—Ä—É—Ç –¥–ª—è –æ—Ç–¥–∞—á–∏ PDF —Ñ–∞–π–ª–æ–≤."""
        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å: –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø—É—Ç—å –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–ø–∞—Å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        if ".." in pdf_filename or pdf_filename.startswith("/") or pdf_filename.startswith("\\"):
            abort(404)
        
        pdf_path = input_files_dir / pdf_filename
        
        if not pdf_path.exists() or not pdf_path.is_file():
            abort(404)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
        if pdf_path.suffix.lower() != ".pdf":
            abort(404)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ input_files_dir
        try:
            pdf_path.resolve().relative_to(_input_files_dir.resolve())
        except ValueError:
            abort(404)
        
        return send_file(pdf_path, mimetype='application/pdf')
    
    # ==================== BBOX Templates API ====================
    
    from bbox_templates import get_template_manager, BboxCoords
    
    @app.route("/api/bbox-templates/suggestions", methods=["GET"])
    def get_bbox_suggestions():
        """
        –ü–æ–ª—É—á–∏—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è bbox –¥–ª—è –∂—É—Ä–Ω–∞–ª–∞.
        
        Query params:
            issn: ISSN –∂—É—Ä–Ω–∞–ª–∞
            page_width: —à–∏—Ä–∏–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã PDF (default 595)
            page_height: –≤—ã—Å–æ—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã PDF (default 842)
        """
        issn = request.args.get("issn", "")
        if not issn:
            return jsonify({"error": "ISSN –Ω–µ —É–∫–∞–∑–∞–Ω"}), 400
        
        page_width = float(request.args.get("page_width", 595))
        page_height = float(request.args.get("page_height", 842))
        
        manager = get_template_manager()
        suggestions = manager.get_suggestions_for_journal(issn, page_width, page_height)
        
        return jsonify(suggestions)
    
    @app.route("/api/bbox-templates/save", methods=["POST"])
    def save_bbox_template():
        """
        –°–æ—Ö—Ä–∞–Ω–∏—Ç—å bbox –∫–∞–∫ –æ–±—Ä–∞–∑–µ—Ü –¥–ª—è —à–∞–±–ª–æ–Ω–∞.
        
        Body JSON:
            issn: ISSN –∂—É—Ä–Ω–∞–ª–∞
            journal_name: –Ω–∞–∑–≤–∞–Ω–∏–µ –∂—É—Ä–Ω–∞–ª–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            field_id: ID –ø–æ–ª—è (title, annotation, references_ru, etc.)
            coords: {page, pdf_x1, pdf_y1, pdf_x2, pdf_y2, page_width, page_height}
        """
        data = request.get_json()
        if not data:
            return jsonify({"error": "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"}), 400
        
        issn = data.get("issn", "")
        if not issn:
            return jsonify({"error": "ISSN –Ω–µ —É–∫–∞–∑–∞–Ω"}), 400
        
        field_id = data.get("field_id", "")
        if not field_id:
            return jsonify({"error": "field_id –Ω–µ —É–∫–∞–∑–∞–Ω"}), 400
        
        coords_data = data.get("coords", {})
        if not coords_data:
            return jsonify({"error": "coords –Ω–µ —É–∫–∞–∑–∞–Ω—ã"}), 400
        
        try:
            coords = BboxCoords(
                page=int(coords_data.get("page", 0)),
                pdf_x1=float(coords_data.get("pdf_x1", 0)),
                pdf_y1=float(coords_data.get("pdf_y1", 0)),
                pdf_x2=float(coords_data.get("pdf_x2", 0)),
                pdf_y2=float(coords_data.get("pdf_y2", 0)),
                page_width=float(coords_data.get("page_width", 595)),
                page_height=float(coords_data.get("page_height", 842)),
            )
        except (ValueError, TypeError) as e:
            return jsonify({"error": f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç: {e}"}), 400
        
        manager = get_template_manager()
        journal_name = data.get("journal_name", "")
        manager.add_bbox_sample(issn, field_id, coords, journal_name)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        suggestions = manager.get_suggestions_for_journal(
            issn, 
            coords.page_width, 
            coords.page_height
        )
        
        return jsonify({
            "success": True,
            "message": f"–®–∞–±–ª–æ–Ω –¥–ª—è {field_id} —Å–æ—Ö—Ä–∞–Ω—ë–Ω",
            "suggestions": suggestions,
        })
    
    @app.route("/api/bbox-templates/list", methods=["GET"])
    def list_bbox_templates():
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —à–∞–±–ª–æ–Ω–æ–≤."""
        manager = get_template_manager()
        templates = manager.list_templates()
        return jsonify({"templates": templates})
    
    @app.route("/api/bbox-templates/delete", methods=["POST"])
    def delete_bbox_template():
        """–£–¥–∞–ª–∏—Ç—å —à–∞–±–ª–æ–Ω –¥–ª—è –∂—É—Ä–Ω–∞–ª–∞."""
        data = request.get_json()
        issn = data.get("issn", "") if data else ""
        
        if not issn:
            return jsonify({"error": "ISSN –Ω–µ —É–∫–∞–∑–∞–Ω"}), 400
        
        manager = get_template_manager()
        success = manager.delete_template(issn)
        
        if success:
            return jsonify({"success": True, "message": f"–®–∞–±–ª–æ–Ω –¥–ª—è {issn} —É–¥–∞–ª—ë–Ω"})
        else:
            return jsonify({"error": "–®–∞–±–ª–æ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω"}), 404
    
    @app.route("/api/bbox-templates/reset-field", methods=["POST"])
    def reset_field_template():
        """–°–±—Ä–æ—Å–∏—Ç—å —à–∞–±–ª–æ–Ω –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–æ–ª—è."""
        data = request.get_json()
        if not data:
            return jsonify({"error": "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"}), 400
        
        issn = data.get("issn", "")
        field_id = data.get("field_id", "")
        
        if not issn:
            return jsonify({"error": "ISSN –Ω–µ —É–∫–∞–∑–∞–Ω"}), 400
        if not field_id:
            return jsonify({"error": "field_id –Ω–µ —É–∫–∞–∑–∞–Ω"}), 400
        
        manager = get_template_manager()
        template = manager.get_template(issn)
        
        if not template:
            return jsonify({"error": "–®–∞–±–ª–æ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω"}), 404
        
        if field_id not in template.fields:
            return jsonify({"error": f"–ü–æ–ª–µ {field_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —à–∞–±–ª–æ–Ω–µ"}), 404
        
        # –£–¥–∞–ª—è–µ–º –ø–æ–ª–µ –∏–∑ —à–∞–±–ª–æ–Ω–∞
        del template.fields[field_id]
        manager.save_template(template)
        
        return jsonify({
            "success": True,
            "message": f"–®–∞–±–ª–æ–Ω –¥–ª—è –ø–æ–ª—è {field_id} —Å–±—Ä–æ—à–µ–Ω",
            "remaining_fields": list(template.fields.keys()),
        })
    
    @app.route("/api/bbox-templates/apply", methods=["POST"])
    def apply_bbox_template():
        """
        –ü—Ä–∏–º–µ–Ω–∏—Ç—å —à–∞–±–ª–æ–Ω –∏ –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ –≤—Å–µ—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π.
        
        Body JSON:
            issn: ISSN –∂—É—Ä–Ω–∞–ª–∞
            pdf_file: –ø—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É
            page_width: —à–∏—Ä–∏–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            page_height: –≤—ã—Å–æ—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            fields: —Å–ø–∏—Å–æ–∫ –ø–æ–ª–µ–π –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –µ—Å–ª–∏ –ø—É—Å—Ç–æ - –≤—Å–µ)
        """
        data = request.get_json()
        if not data:
            return jsonify({"error": "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"}), 400
        
        issn = data.get("issn", "")
        pdf_file = data.get("pdf_file", "")
        
        if not issn:
            return jsonify({"error": "ISSN –Ω–µ —É–∫–∞–∑–∞–Ω"}), 400
        if not pdf_file:
            return jsonify({"error": "PDF —Ñ–∞–π–ª –Ω–µ —É–∫–∞–∑–∞–Ω"}), 400
        
        page_width = float(data.get("page_width", 595))
        page_height = float(data.get("page_height", 842))
        fields_filter = data.get("fields", [])
        
        manager = get_template_manager()
        suggestions = manager.get_suggestions_for_journal(issn, page_width, page_height)
        
        if not suggestions.get("suggestions"):
            return jsonify({"error": "–ù–µ—Ç —à–∞–±–ª–æ–Ω–æ–≤ –¥–ª—è —ç—Ç–æ–≥–æ –∂—É—Ä–Ω–∞–ª–∞"}), 404
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—É—Ç—å –∫ PDF
        if ".." in pdf_file or pdf_file.startswith("/") or pdf_file.startswith("\\"):
            return jsonify({"error": "–ù–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É"}), 400
        
        pdf_path = _input_files_dir / pdf_file
        if not pdf_path.exists():
            return jsonify({"error": f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {pdf_file}"}), 404
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –∫–∞–∂–¥–æ–π –æ–±–ª–∞—Å—Ç–∏
        extracted = {}
        try:
            import pdfplumber
            with pdfplumber.open(str(pdf_path)) as pdf:
                total_pages = len(pdf.pages)
                
                for field_id, suggestion in suggestions["suggestions"].items():
                    if fields_filter and field_id not in fields_filter:
                        continue
                    
                    coords = suggestion["coords"]
                    page_num = coords["page"]
                    
                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤ —Å—Ç—Ä–∞–Ω–∏—Ü
                    if page_num < 0:
                        page_num = total_pages + page_num
                    
                    if page_num < 0 or page_num >= total_pages:
                        continue
                    
                    page = pdf.pages[page_num]
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
                    try:
                        cropped = page.crop((
                            coords["pdf_x1"],
                            coords["pdf_y1"],
                            coords["pdf_x2"],
                            coords["pdf_y2"]
                        ))
                        text = cropped.extract_text(x_tolerance=3, y_tolerance=5)
                        
                        if text:
                            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç
                            options = {
                                "fix_hyphenation": True,
                                "strip_prefix": True,
                                "join_lines": field_id not in {"references_ru", "references_en"},
                            }
                            text = _normalize_extracted_text(text, field_id, options)
                            
                            extracted[field_id] = {
                                "text": text,
                                "confidence": suggestion["confidence"],
                                "page": page_num,
                            }
                    except Exception as e:
                        print(f"Error extracting {field_id}: {e}")
                        continue
        
        except Exception as e:
            return jsonify({"error": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏: {str(e)}"}), 500
        
        return jsonify({
            "success": True,
            "issn": issn,
            "pdf_file": pdf_file,
            "extracted": extracted,
        })

